{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "EIUmjkUQVPgG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "from random import randrange\n",
        "import shutil\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the path for the 'dataset' directory\n",
        "dataset_dir = os.path.join(\"\", \"dataset\")\n",
        "# Create the 'dataset' directory if it doesn't exist\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "# Print the list of files and directories in the 'dataset' directory\n",
        "print(os.listdir(dataset_dir))\n",
        "\n",
        "# Create the path for the 'updatedDataset' directory\n",
        "updated_dataset_dir = os.path.join(\"\", \"updatedDataset\")\n",
        "# Create the 'updatedDataset' directory if it doesn't exist\n",
        "os.makedirs(updated_dataset_dir, exist_ok=True)\n",
        "# Print the list of files and directories in the 'updatedDataset' directory\n",
        "print(os.listdir(updated_dataset_dir))\n",
        "\n",
        "# 440 format dimensions for height and width\n",
        "H = 440.0\n",
        "B = 440.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFVIYeA8VWgc",
        "outputId": "6d666a18-991c-4d78-92c1-24cce5e19e6d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_6.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_11.jpg', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_7.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_14.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_9.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_10.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_0.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_1.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_9.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_9.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall2_Time1_Camera_0.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_5.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_9.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_11.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_7.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_5.txt', '.DS_Store', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_0.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_5.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_9.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_12.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_8.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_14.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_10.jpg', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_9.jpg', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_5.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_12.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall2_Time1_Camera_0.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_8.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_6.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_1.jpg']\n",
            "['Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_6.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_11.jpg', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_7.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_14.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_9.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_10.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_0.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_1.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_9.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_9.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall2_Time1_Camera_0.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_5.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_9.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_11.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_7.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_5.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_0.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_5.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_9.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_12.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_8.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_14.txt', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_10.jpg', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_9.jpg', 'Screenshot_Room7_LeftWall_Bathtub1_Faucet_Bathtub1_Floor1_Wall1_Time0_Camera_5.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_12.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall2_Time1_Camera_0.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time1_Camera_8.jpg', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_6.txt', 'Screenshot_Room7_LeftWall_Toilet1_FlushHandle1_Floor1_Wall1_Time3_Camera_1.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_names(folder_path, format=\".txt\"):\n",
        "    # Check if the format is \".txt\"\n",
        "    if format == \".txt\":\n",
        "        # List comprehension to find all files ending with '.txt' in the specified folder\n",
        "        txt_labels = [file_name for file_name in os.listdir(folder_path) if file_name.endswith('.txt')]\n",
        "        # Return the list of txt file names\n",
        "        return txt_labels\n",
        "\n",
        "def check_update_outliers(updated_x, updated_y, side_size_x, side_size_y):\n",
        "    # Check for the corners where both X and Y boundaries are exceeded\n",
        "    # Bottom-left corner\n",
        "    if (updated_x - side_size_x / 2 < 0) and (updated_y - side_size_y / 2 < 0):\n",
        "        # Adjust both x and y coordinates to bring the bounding box within the boundary\n",
        "        updated_x = side_size_x / 2\n",
        "        updated_y = side_size_y / 2\n",
        "\n",
        "    # Bottom-right corner\n",
        "    if (updated_x + side_size_x / 2 > 1) and (updated_y - side_size_y / 2 < 0):\n",
        "        # Adjust both x and y coordinates to bring the bounding box within the boundary\n",
        "        updated_x = 1 - side_size_x / 2\n",
        "        updated_y = side_size_y / 2\n",
        "\n",
        "    # Top-left corner\n",
        "    if (updated_x - side_size_x / 2 < 0) and (updated_y + side_size_y / 2 > 1):\n",
        "        # Adjust both x and y coordinates to bring the bounding box within the boundary\n",
        "        updated_x = side_size_x / 2\n",
        "        updated_y = 1 - side_size_y / 2\n",
        "\n",
        "    # Top-right corner\n",
        "    if (updated_x + side_size_x / 2 > 1) and (updated_y + side_size_y / 2 > 1):\n",
        "        # Adjust both x and y coordinates to bring the bounding box within the boundary\n",
        "        updated_x = 1 - side_size_x / 2\n",
        "        updated_y = 1 - side_size_y / 2\n",
        "\n",
        "    # Check if the right edge of the bounding box exceeds the right boundary (1.0)\n",
        "    if updated_x + side_size_x / 2 > 1:\n",
        "        # Adjust the x coordinate to bring the bounding box within the boundary\n",
        "        updated_x = updated_x - (updated_x + side_size_x / 2 - 1) / 2\n",
        "        # Adjust the width of the bounding box to fit within the boundary\n",
        "        side_size_x = 2 * (1 - updated_x)\n",
        "    # Check if the left edge of the bounding box exceeds the left boundary (0.0)\n",
        "    elif updated_x - side_size_x / 2 < 0:\n",
        "        # Adjust the x coordinate to bring the bounding box within the boundary\n",
        "        updated_x = updated_x + (side_size_x / 2 - updated_x) / 2\n",
        "        # Adjust the width of the bounding box to fit within the boundary\n",
        "        side_size_x = 2 * updated_x\n",
        "\n",
        "    # Check if the top edge of the bounding box exceeds the top boundary (1.0)\n",
        "    if updated_y + side_size_y / 2 > 1:\n",
        "        # Adjust the y coordinate to bring the bounding box within the boundary\n",
        "        updated_y = updated_y - (updated_y + side_size_y / 2 - 1) / 2\n",
        "        # Adjust the height of the bounding box to fit within the boundary\n",
        "        side_size_y = 2 * (1 - updated_y)\n",
        "    # Check if the bottom edge of the bounding box exceeds the bottom boundary (0.0)\n",
        "    elif updated_y - side_size_y / 2 < 0:\n",
        "        # Adjust the y coordinate to bring the bounding box within the boundary\n",
        "        updated_y = updated_y + (side_size_y / 2 - updated_y) / 2\n",
        "        # Adjust the height of the bounding box to fit within the boundary\n",
        "        side_size_y = 2 * updated_y\n",
        "\n",
        "    # Return the potentially adjusted coordinates and sizes\n",
        "    return updated_x, updated_y, side_size_x, side_size_y\n",
        "\n",
        "def update_normalize_xy(param, side_size, dimension=\"X\", H=440., B=440.):\n",
        "    \"\"\"\n",
        "    Normalizes the param value for the specified dimension (X or Y).\n",
        "\n",
        "    Args:\n",
        "    param (float): The parameter value to normalize.\n",
        "    side_size (float): The size to be added for centering.\n",
        "    dimension (str): The dimension to normalize ('X' or 'Y').\n",
        "    H (float): The maximum range for X dimension (default is 3840.0).\n",
        "    B (float): The maximum range for Y dimension (default is 2160.0).\n",
        "\n",
        "    Returns:\n",
        "    float: The normalized parameter value in the range [0, 1].\n",
        "\n",
        "    Raises:\n",
        "    ValueError: If the dimension is not 'X' or 'Y'.\n",
        "    \"\"\"\n",
        "    if dimension == \"X\":\n",
        "        # Normalize the param value for the X dimension\n",
        "        return (param + side_size / 2) / H\n",
        "    elif dimension == \"Y\":\n",
        "        # Normalize the param value for the Y dimension\n",
        "        return (param + side_size / 2) / B\n",
        "    else:\n",
        "        raise ValueError(\"Dimension must be 'X' or 'Y'\")\n",
        "\n",
        "def change_label_content(path, updated_path):\n",
        "    # Initialize variables to hold label index, coordinates, and sizes\n",
        "    index = 0\n",
        "    updated_x = 0\n",
        "    updated_y = 0\n",
        "    side_size_x = 0\n",
        "    side_size_y = 0\n",
        "\n",
        "    sts = []\n",
        "\n",
        "    # Open the file at the given path in read mode\n",
        "    with open(path, \"r\") as fileref:\n",
        "        # Iterate over each line in the file\n",
        "        for i in fileref:\n",
        "            # Split the line into parts and parse the values as floats\n",
        "            name = i.split(\" \")[0]\n",
        "            index1 = (i.split(\" \")[1])\n",
        "            index2 = (i.split(\" \")[2])\n",
        "            updated_x = float(i.split(\" \")[3])\n",
        "            updated_y = float(i.split(\" \")[4])\n",
        "            side_size_x = float(i.split(\" \")[5])\n",
        "            side_size_y = float(i.split(\" \")[6])\n",
        "\n",
        "            # Normalize the x and y coordinates\n",
        "            updated_x = update_normalize_xy(updated_x, side_size_x, \"X\")\n",
        "            updated_y = update_normalize_xy(updated_y, side_size_y, \"Y\")\n",
        "\n",
        "            # Normalize the side sizes (assuming H and B are global or defined elsewhere)\n",
        "            side_size_x = side_size_x / H\n",
        "            side_size_y = side_size_y / B\n",
        "\n",
        "            updated_x, updated_y, side_size_x, side_size_y = check_update_outliers(updated_x, updated_y, side_size_x, side_size_y)\n",
        "\n",
        "            # Create a string with updated values concatenated\n",
        "            st = f\"{name} {index1} {index2} {updated_x} {updated_y} {side_size_x} {side_size_y}\"\n",
        "            sts.append(st)\n",
        "\n",
        "    # Open the file at the given path in write mode and write the updated string\n",
        "    with open(updated_path, \"w\") as file:\n",
        "      print(sts)\n",
        "      file.writelines(\"\\n\".join(sts))"
      ],
      "metadata": {
        "id": "m4nEWxdzagGY"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_jpg_images(folder_path, format=\".jpg\"):\n",
        "    # Initialize counters for jpg and txt files\n",
        "    jpg_count = 0\n",
        "    txt_count = 0\n",
        "\n",
        "    # Check the format parameter to determine which files to count\n",
        "    if format == \".jpg\":\n",
        "        # Loop through all files in the specified folder\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            # Check if the file name ends with '.jpg'\n",
        "            if file_name.endswith('.jpg'):\n",
        "                # Increment the jpg file count\n",
        "                jpg_count += 1\n",
        "        # Return the count of jpg files\n",
        "        return jpg_count\n",
        "    elif format == \".txt\":\n",
        "        # Loop through all files in the specified folder\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            # Check if the file name ends with '.txt'\n",
        "            if file_name.endswith('.txt'):\n",
        "                # Increment the txt file count\n",
        "                txt_count += 1\n",
        "        # Return the count of txt files\n",
        "        return txt_count\n",
        "\n",
        "def get_jpg_image_names(folder_path, format=\".jpg\"):\n",
        "    # Check if the format is \".jpg\"\n",
        "    if format == \".jpg\":\n",
        "        # List comprehension to find all files ending with '.jpg' in the specified folder\n",
        "        jpg_images = [file_name for file_name in os.listdir(folder_path) if file_name.endswith('.jpg')]\n",
        "        # Return the list of jpg file names\n",
        "        return jpg_images\n",
        "    else:\n",
        "        # List comprehension to find all files ending with '.txt' in the specified folder\n",
        "        txt_images = [file_name for file_name in os.listdir(folder_path) if file_name.endswith('.txt')]\n",
        "        # Return the list of txt file names\n",
        "        return txt_images\n",
        "\n",
        "def copy_image(src_folder, dest_folder, image_name):\n",
        "    # Ensure the destination folder exists\n",
        "    os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "    # Define the full file paths\n",
        "    src_path = os.path.join(src_folder, image_name)\n",
        "    dest_path = os.path.join(dest_folder, image_name)\n",
        "\n",
        "    # Copy the image\n",
        "    shutil.copy(src_path, dest_path)\n",
        "\n",
        "def copy_file(src_folder, dest_folder, file_name):\n",
        "    # Ensure the destination folder exists\n",
        "    os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "    # Define the full file paths\n",
        "    src_path = os.path.join(src_folder, file_name)\n",
        "    dest_path = os.path.join(dest_folder, file_name)\n",
        "\n",
        "    # Copy the file\n",
        "    shutil.copy(src_path, dest_path)\n",
        "\n",
        "def copy_all_images(src_folder, dest_folder, image_names):\n",
        "  for image_name in image_names:\n",
        "     copy_image(src_folder, dest_folder, image_name)\n",
        "\n",
        "def split_data():\n",
        "    # Define directories for dataset, training images, validation images, training labels, and validation labels\n",
        "    dataset_dir = 'updatedDataset'\n",
        "    image_train_dir = 'images/train'\n",
        "    image_val_dir = 'images/val'\n",
        "    labels_train_dir = 'labels/train'\n",
        "    labels_val_dir = 'labels/val'\n",
        "\n",
        "    # Count the number of jpg images in the dataset directory\n",
        "    number_of_images = count_jpg_images(dataset_dir)\n",
        "\n",
        "    # Get the list of jpg image names in the dataset directory\n",
        "    image_names = get_jpg_image_names(dataset_dir)\n",
        "    # Dictionary to store image names and their corresponding random index\n",
        "    image_names_index = {}\n",
        "\n",
        "    # Get the list of txt label names in the dataset directory\n",
        "    label_names = get_jpg_image_names(dataset_dir, \".txt\")\n",
        "    # Dictionary to store label names and their corresponding random index\n",
        "    label_names_index = {}\n",
        "\n",
        "    # Lists to store the names of the labels for training and validation sets\n",
        "    labels_names_train = []\n",
        "    labels_names_val = []\n",
        "\n",
        "    # Add image and label names into the dictionaries with random index values\n",
        "    for i in range(len(image_names)):\n",
        "        image_names_index[image_names[i]] = randrange(10)\n",
        "        label_names_index[label_names[i]] = image_names_index[image_names[i]]\n",
        "\n",
        "    # Split images and labels into training and validation sets\n",
        "    for i in range(number_of_images):\n",
        "        # If the random index is 9 or 0, consider it for validation set\n",
        "        if list(image_names_index.values())[i] == 9 or list(image_names_index.values())[i] == 0:\n",
        "            # Copy image to validation directory\n",
        "            copy_image(dataset_dir, image_val_dir, list(image_names_index.keys())[i])\n",
        "            # Copy label to validation directory\n",
        "            copy_file(dataset_dir, labels_val_dir, list(label_names_index.keys())[i])\n",
        "            # Add label name to validation list\n",
        "            labels_names_val.append(list(label_names_index.keys())[i])\n",
        "        else:\n",
        "            # Otherwise, consider it for training set\n",
        "            # Copy image to training directory\n",
        "            copy_image(dataset_dir, image_train_dir, list(image_names_index.keys())[i])\n",
        "            # Copy label to training directory\n",
        "            copy_file(dataset_dir, labels_train_dir, list(label_names_index.keys())[i])\n",
        "            # Add label name to training list\n",
        "            labels_names_train.append(list(label_names_index.keys())[i])\n",
        "\n",
        "    # Return the lists of validation and training label names\n",
        "    return labels_names_val, labels_names_train\n",
        "\n",
        "\n",
        "def update_label_id(label_names, path, product_type_id):\n",
        "    names = {}\n",
        "    # Loop through each label name in the provided list\n",
        "    for index in range(len(label_names)):\n",
        "        # Open the current label file in read mode\n",
        "        with open(path + \"/\" + label_names[index], \"r\") as fileref:\n",
        "            # Initialize an empty string to store the new label content\n",
        "            st = \"\"\n",
        "            # Read each line in the file\n",
        "            for i in fileref:\n",
        "                # Check if the second element in the line matches the product_type_id\n",
        "                if product_type_id == int(i.split()[1]):\n",
        "\n",
        "                    # Update the string with the elements after the second one\n",
        "                    st_helper1= int(i.split()[2]) - 1\n",
        "                    st_helper2 = \" \".join(i.split()[3:])\n",
        "                    st = (str(st_helper1) + \" \" + str(st_helper2))\n",
        "\n",
        "                    names[st_helper1] = i.split()[0]\n",
        "                else:\n",
        "                    if not st: # if the string is empty\n",
        "                        st = \"\" # string should stay empty\n",
        "\n",
        "        # Open the same file in write mode to update its content\n",
        "        with open(path + \"/\" + label_names[index], 'w') as file:\n",
        "            # Write the updated string to the file\n",
        "            file.write(st)\n",
        "    print(names)\n",
        "    return names\n",
        "\n",
        "def update_label_names(label_names, path, product_type_list_names=[]):\n",
        "    # Initialize an empty string to store the new label content\n",
        "    st = \"\"\n",
        "    names = {}\n",
        "\n",
        "    # save all groups in list\n",
        "    all_groups = [] # liste mit neuen namen\n",
        "    for element_name in product_type_list_names:\n",
        "        all_groups.append(element_name)\n",
        "\n",
        "    # Create a dictionary that maps each group to an integer, so that we start counting from 0\n",
        "    newLabel_from0 = {group: index for index, group in enumerate(all_groups)}\n",
        "    newGroup_from0 = {index: group for group, index in newLabel_from0.items()}\n",
        "\n",
        "    # Loop through each label name in the provided list\n",
        "    for index in range(len(label_names)):\n",
        "        # Open the current label file in read mode\n",
        "        with open(path + \"/\" + label_names[index], \"r\") as fileref:\n",
        "            # Read each line in the file\n",
        "            for i in fileref:\n",
        "                # Check if the second element in the line matches the product_type_id\n",
        "                for j in product_type_list_names: # Sink\n",
        "                    if products[j] == int(i.split()[1]):\n",
        "\n",
        "                        newID = newLabel_from0.get(j)\n",
        "                        st_helper1 = str(newID)\n",
        "                        # Update the string with the elements after the third one\n",
        "                        st_helper2 = \" \".join(i.split()[3:])\n",
        "                        # Update the string with the elements after the third one\n",
        "                        st = (str(st_helper1) + \" \" + str(st_helper2))\n",
        "\n",
        "                        names[newID] = newGroup_from0[newID]\n",
        "                    else:\n",
        "                        if not st: # if the string is empty\n",
        "                            st = \"\" # string should stay empty\n",
        "\n",
        "\n",
        "        # Open the same file in write mode to update its content\n",
        "        with open(path + \"/\" + label_names[index], 'w') as file:\n",
        "            # Write the updated string to the file\n",
        "            file.write(st)\n",
        "    print(names)\n",
        "    return names\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_yolo_training_config(folder_path, product_specific=False, product_type_id=None, product_type_list_names=None):\n",
        "    # Ensure the path exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    # Create the folder structure\n",
        "    images_train_dir = os.path.join('images', 'train')\n",
        "    images_val_dir = os.path.join('images', 'val')\n",
        "    labels_train_dir = os.path.join('labels', 'train')\n",
        "    labels_val_dir = os.path.join('labels', 'val')\n",
        "\n",
        "    os.makedirs(images_train_dir, exist_ok=True)\n",
        "    os.makedirs(images_val_dir, exist_ok=True)\n",
        "    os.makedirs(labels_train_dir, exist_ok=True)\n",
        "    os.makedirs(labels_val_dir, exist_ok=True)\n",
        "\n",
        "    dataset_dir = os.path.join(\"\", \"updatedDataset\")\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    labels_names_val, labels_names_train = split_data()\n",
        "\n",
        "    products = {\"Toilet\":0, \"Bathtub\":1, \"Sink\":2}\n",
        "\n",
        "    if product_specific==True:\n",
        "        # Update label IDs for validation and training sets\n",
        "        names = update_label_id(labels_names_val, labels_val_dir, product_type_id) ### list names hinzufügen\n",
        "        names = update_label_id(labels_names_train, labels_train_dir, product_type_id)\n",
        "    else:\n",
        "        # Update label IDs for validation and training sets\n",
        "        names = update_label_names(labels_names_val, labels_val_dir, product_type_list_names)\n",
        "        names = update_label_names(labels_names_train, labels_train_dir, product_type_list_names)\n",
        "\n",
        "    # Define the YAML configuration structure\n",
        "    yolo_config = OrderedDict([\n",
        "      ('path', '../updatedDataset'),\n",
        "      ('train', 'images/train'),\n",
        "      ('val', 'images/val'),\n",
        "      ('test', ''),\n",
        "      ('names', names)\n",
        "    ])\n",
        "\n",
        "    # Function to represent OrderedDict as a standard YAML dictionary\n",
        "    def represent_ordereddict(dumper, data):\n",
        "        return dumper.represent_mapping('tag:yaml.org,2002:map', data.items())\n",
        "\n",
        "    yaml.add_representer(OrderedDict, represent_ordereddict)\n",
        "\n",
        "\n",
        "    # Write the configuration to a YAML file\n",
        "    with open('data.yaml', 'w') as file:\n",
        "        yaml.dump(yolo_config, file, default_flow_style=False)\n",
        "\n",
        "    print(\"YAML configuration file has been generated.\")\n"
      ],
      "metadata": {
        "id": "AaYw9chKclZA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_label_names(dataset_dir, format=\".txt\")\n",
        "for i in get_label_names(dataset_dir, format=\".txt\"):\n",
        "  path = dataset_dir + \"/\" + i\n",
        "  path = \"dataset\" + \"/\" + i\n",
        "  updated_path = \"updatedDataset\" + \"/\" + i\n",
        "  change_label_content(path, updated_path)\n",
        "  #copy images in folder with normalized labels\n",
        "copy_all_images(dataset_dir, updated_dataset_dir, get_jpg_image_names(dataset_dir))"
      ],
      "metadata": {
        "id": "mUYhrCdqaqIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0ba111-5a92-4192-8f4a-765451daf1a5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Toilet1 0 1 0.0012900098863636363 0.0014220813636363634 0.0003835756818181818 0.00035763727272727274', 'FlushHandle1 5 1 0.001099989659090909 0.0006793239204545454 0.00022926931818181818 0.0001774114772727273', 'Brush1 5 1 0.00079852925 0.001339199318181818 8.724259090909092e-05 0.0002117740909090909', 'Paper1 5 1 0.0013711785227272727 0.0009208115454545455 0.00012000431818181819 7.757581818181818e-05']\n",
            "['Bathtub1 1 1 0.5571642045454546 0.4730803409090909 0.6203611363636364 0.2151043181818182', 'Faucet_Bathtub1 1 1 0.539024090909091 0.3370196136363637 0.07775272727272727 0.04848195454545454']\n",
            "['Toilet1 0 1 0.001414629090909091 0.0017312573863636361 0.0007144563636363637 0.0006469529545454545', 'FlushHandle1 5 1 0.0010731745454545453 0.00039451397727272724 0.0003758186363636364 0.0002751838636363637', 'Brush1 5 1 0.0004716531931818182 0.001616247840909091 0.00014105411363636363 0.00043344704545454545', 'Paper1 5 1 0.001560059625 0.0008095992045454545 0.00021334152272727274 0.00013372295454545455']\n",
            "['Bathtub1 1 1 0.5029531136363636 0.46609282954545456 0.5791186363636364 0.2058806590909091', 'Faucet_Bathtub1 1 1 0.49472042045454545 0.3341231704545455 0.06380538636363636 0.04508361363636364']\n",
            "['Toilet1 0 1 0.0012900098863636363 0.0014220813636363634 0.0003835756818181818 0.00035763727272727274', 'FlushHandle1 5 1 0.001099989659090909 0.0006793239204545454 0.00022926931818181818 0.0001774114772727273', 'Brush1 5 1 0.00079852925 0.001339199318181818 8.724259090909092e-05 0.0002117740909090909', 'Paper1 5 1 0.0013711785227272727 0.0009208115454545455 0.00012000431818181819 7.757581818181818e-05']\n",
            "['Toilet1 0 1 0.0015538394318181817 0.0014725604545454546 0.0005323420454545455 0.00039204227272727273', 'FlushHandle1 5 1 0.001674725340909091 0.0006859466136363636 0.00027440113636363633 0.00022288549999999999', 'Brush1 5 1 0.0012280327613636364 0.0012113144204545453 6.500006818181818e-05 0.0001810879318181818', 'Paper1 5 1 0.001950365340909091 0.0011304389659090909 0.0002469129545454545 0.00015591747727272727']\n",
            "['Bathtub1 1 1 0.553772159090909 0.47085504545454543 0.7742877272727273 0.5316720454545455', 'Faucet_Bathtub1 1 1 0.5999988863636363 0.24374587500000003 0.18933413636363636 0.10897806818181818']\n",
            "['Toilet1 0 1 0.0013911076136363634 0.0016180369318181819 0.0006679706818181818 0.0006266902272727273', 'FlushHandle1 5 1 0.0010732476136363636 0.0003516927272727273 0.0003817529545454545 0.0002838827272727273', 'Brush1 5 1 0.0005258983068181818 0.0014890474999999998 0.0001353729772727273 0.0003775036363636363', 'Paper1 5 1 0.0015503683068181816 0.000745781534090909 0.00020837752272727272 0.0001491289772727273']\n",
            "['Bathtub1 1 1 0.6728592727272728 0.43067268181818175 0.6542814545454545 0.48062636363636363', 'Faucet_Bathtub1 1 1 0.42926149999999996 0.19138201136363636 0.1471089090909091 0.08472297727272726']\n",
            "['Toilet1 0 1 0.001241961590909091 0.0014853239772727274 0.000539665 0.0004001925', 'FlushHandle1 5 1 0.0008475310227272726 0.0006870378409090908 0.0002928156818181818 0.0002305384090909091', 'Brush1 5 1 0.000632082340909091 0.0015818805681818181 0.00016065604545454544 0.00025855795454545453', 'Paper1 5 1 0.001131000784090909 0.000912921659090909 0.00012100338636363636 8.010422727272727e-05']\n",
            "['Toilet1 0 1 0.0013358250000000001 0.0013450973863636364 0.00034887454545454546 0.0003378193181818182', 'FlushHandle1 5 1 0.0013726053863636362 0.0006648509659090909 0.00022644940909090908 0.00018122965909090908', 'Brush1 5 1 0.0010137945681818182 0.0011535183977272726 5.4921863636363635e-05 0.00016412088636363634', 'Paper1 5 1 0.0015978387613636363 0.0009741678636363636 0.0001226311590909091 0.00010848527272727273']\n",
            "['Bathtub1 1 1 0.5255468181818181 0.43732784090909094 0.9303313636363636 0.40118386363636366', 'Faucet_Bathtub1 1 1 0.49072170454545455 0.21127753409090907 0.11622522727272727 0.09706011363636363']\n",
            "['Toilet1 0 1 0.0012294832954545453 0.0014037963636363635 0.00047842204545454545 0.0003617918181818182', 'FlushHandle1 5 1 0.0008531198863636364 0.0007254061363636364 0.0002862915909090909 0.00023283818181818184', 'Brush1 5 1 0.0007175458181818181 0.001477560215909091 0.00013999572727272727 0.00018551770454545456', 'Paper1 5 1 0.0011311653636363636 0.0008778351363636363 0.00011739163636363636 8.622209090909091e-05']\n",
            "['Toilet1 0 1 0.0013292829545454546 0.0018322857954545457 0.0009945322727272726 0.0007207738636363637', 'FlushHandle1 5 1 0.0006452517045454545 0.0004037797727272727 0.0004045297727272727 0.0003340581818181818', 'Brush1 5 1 0.00017359427272727274 0.002007196363636364 0.0002128116818181818 0.0005440640909090909', 'Paper1 5 1 0.0011265325681818182 0.0007987568636363637 0.00022067604545454545 0.00013923236363636365']\n",
            "['Toilet1 0 1 0.001523652840909091 0.0016114188636363637 0.0006717638636363636 0.0006234636363636364', 'FlushHandle1 5 1 0.0015274082954545454 0.0003490121818181818 0.0003739415909090909 0.00028196886363636366', 'Brush1 5 1 0.0008898797386363637 0.001297520909090909 0.00010940629545454545 0.00034263090909090907', 'Paper1 5 1 0.001966361931818182 0.0008687020000000001 0.0002143315909090909 0.00017855036363636365']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "products = {\"Toilet\":0, \"Bathtub\":1, \"Sink\":2}\n",
        "create_yolo_training_config(\"updatedDataset\", product_specific=False, product_type_list_names=[\"Toilet\", \"Bathtub\", \"Sink\"])\n",
        "#create_yolo_training_config(\"updatedDataset\", product_specific=True, product_type_id=products[\"Toilet\"])\n"
      ],
      "metadata": {
        "id": "7dn2_KCUazlz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47351ca-425d-4931-d5cb-2cc735fdfaae"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'Bathtub', 0: 'Toilet'}\n",
            "{0: 'Toilet', 1: 'Bathtub'}\n",
            "YAML configuration file has been generated.\n"
          ]
        }
      ]
    }
  ]
}